{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hdbcli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from hdbcli import dbapi\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = dbapi.connect(\n",
    "    # key='USER1UserKey', \n",
    "    address='10.130.222.132',\n",
    "    port='34216',\n",
    "    user='PROCESSAND',\n",
    "    password='Delhi-123-cocoon-jape',\n",
    "    encrypt=True, \n",
    "    sslValidateCertificate=False \n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_duplicates(schema, table, code_only = False, use_csv_path = None ,sep=\",\"):\n",
    "    print(f\"-- Schema name: {schema}, Table name: {table}\")\n",
    "    \n",
    "    # code_only: To only print out the SQL code\n",
    "    # use_csv_path: Enter the file path of DD03M.csv\n",
    "    # sep: Delimiter of the csv file \n",
    "    ## DD03M  csv file must include columns: TABNAME, FIELDNAME, KEYFLAG\n",
    "\n",
    "    keys = []\n",
    "    sql1 = f\"SELECT DISTINCT FIELDNAME FROM {schema}.DD03M WHERE KEYFLAG='X' AND TABNAME = '{table}';\"\n",
    "\n",
    "    if code_only:\n",
    "        # Import DD03M from csv file\n",
    "        if use_csv_path is not None:\n",
    "            df1 =pd.read_csv(use_csv_path, sep=sep)\n",
    "            csv_keys = df1.loc[(df1['TABNAME']==table) & (df1['KEYFLAG']==\"X\")]['FIELDNAME'].unique()\n",
    "            for key in csv_keys:\n",
    "                keys.append(str(key))\n",
    "            if len(keys)==0:\n",
    "                raise Exception(\"Error: Table or column missing in the csv file.\")\n",
    "        else:\n",
    "            raise Exception(\"Error: Please enter csv file path for offline use.\")\n",
    "    # Import DD03M from shema\n",
    "    else:        \n",
    "        cursor.execute(sql1)\n",
    "        for key in cursor:\n",
    "            keys.append(str(key[0]))\n",
    "  \n",
    "    print(\"-- Primary keys: \" + \",\".join(keys))\n",
    "    _case_key = '||'.join(keys)\n",
    "\n",
    "    # Count duplicated cases in the target table\n",
    "    sql2 = f\"SELECT COUNT(1) - COUNT(DISTINCT {_case_key}) FROM {schema}.{table};\"\n",
    "    if code_only:\n",
    "        print(f\"--Calculate duplicates for {schema}.{table}\")\n",
    "        print(sql2)\n",
    "    else:\n",
    "        cursor.execute(sql2)\n",
    "        if cursor is None :\n",
    "            print(\"No Duplicates\")\n",
    "        else: \n",
    "            print(\"Duplicated rows count: \" + str(cursor.fetchall()[0][0]))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_duplicates_multi_tables(schemas:list, tables:list, code_only = None, use_csv_path = None, sep = \",\"):\n",
    "    for schema in schemas:\n",
    "        for table in tables:\n",
    "            try:\n",
    "                show_duplicates(schema,table,code_only,use_csv_path,sep)\n",
    "                print(\"\")\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dulplicates(target_schema, source_schema, table, keys:list, sorting_col:list, code_only=False):\n",
    "    \n",
    "    key_str = ','.join(keys)\n",
    "    sorting_str = \",\".join(sorting_col)\n",
    "    print(f\"--Target schema: {target_schema}\")\n",
    "    print(f\"--Source schema: {source_schema}\")\n",
    "    print(f\"--Table: {table}\")\n",
    "    print(\"--Primary keys: \" + key_str)\n",
    "    print(\"--Sorting columns: \" + sorting_str)\n",
    "\n",
    "    # # Drop the BACKUP table alreaydy exists\n",
    "    # drop_if_exist = f\"CALL {target_schema}.DROP_IF_EXISTS('{target_schema}', '{table}_BACKUP');\"\n",
    "    \n",
    "    # Copy and create the backup table and add ROW_NUMBER()\n",
    "    \n",
    "    create_table = f'CREATE TABLE \"{target_schema}\".\"{table}_BACKUP\" AS (SELECT ROW_NUMBER() OVER(PARTITION BY {key_str} ORDER BY {sorting_str}) AS _ROW_NUMBER, *  FROM {source_schema}.{table})'\n",
    "\n",
    "    count_rows = f'SELECT _ROW_NUMBER, COUNT(1) FROM \"{target_schema}\".\"{table}_BACKUP\" GROUP BY _ROW_NUMBER'\n",
    "\n",
    "    remove_rows = f'DELETE FROM \"{target_schema}\".\"{table}_BACKUP\" WHERE _ROW_NUMBER >= 2'\n",
    "    \n",
    "    if code_only:\n",
    "        # print(\"-- Drop BACKUP table if exists\")\n",
    "        # print(drop_if_exist)\n",
    "        print(\"-- Create BACKUP table in the target schema\")\n",
    "        print(create_table)\n",
    "        print(\"-- Duplicated rows will have _ROW_NUMBER >= 2\")\n",
    "        print(count_rows)\n",
    "        print(\"-- Remove duplicated rows from the BACKUP table\")\n",
    "        print(remove_rows)\n",
    "    else:\n",
    "        # cursor.execute(drop_if_exist)    \n",
    "        cursor.execute(create_table)\n",
    "        cursor.execute(count_rows)\n",
    "\n",
    "        duplicated_rows = 0\n",
    "        for row in cursor.fetchall()[1:]:\n",
    "            duplicated_rows += row[1]\n",
    "        print(f\"Schema: {source_schema}, Table: {table}, Duplicated rows: \" + str(row[1]))\n",
    "\n",
    "        # Remove duplicate records from the backup table\n",
    "        cursor.execute(remove_rows)  \n",
    "        print(f\"Duplicates removed in {target_schema}.{table}_BACKUP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1\n",
    "show_duplicates_multi_tables(['ABAP_P60', 'P1A_CONSOLIDATED'], tables=['EKKO','EKPO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "show_duplicates_multi_tables(['ABAP_P60', 'P1A_CONSOLIDATED'], tables=['EKKO','EKPO'], code_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_duplicates_multi_tables(['ABAP_P60', 'P1A_CONSOLIDATED'], tables=['EKKO'], code_only=True, use_csv_path = \"C:/Users/KencoWu/Downloads/results_1.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "show_duplicates_multi_tables(['ABAP_P60', 'P1A_CONSOLIDATED'], tables=['EKKO','EKPO'], code_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 4\n",
    "remove_dulplicates(\"PROCESSAND\", \"ABAP_P60\", \"EKKO\", [\"MANDT\", \"EBELN\"], [\"AEDAT\", \"BUKRS\"], code_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 5\n",
    "remove_dulplicates(\"PROCESSAND\", \"ABAP_P60\", \"EKKO\", [\"MANDT\", \"EBELN\"], [\"AEDAT\", \"BUKRS\"], code_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}